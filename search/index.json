[{"content":"AI is rapidly evolving. This is a known fact and should come as no surprise. The next frontier will likely emerge in the agentic space, focused on fully working out how to make agents do the bidding of their overlords. As of early 2026, the latest trend is Agent Skills — a powerful mechanism that allows you to define a structured set of instructions within a given context or framework, controlling how an agent performs a specific set of tasks.\nBundled into this new AI-driven world that we live in is the software we use everyday. In the realm of software engineering, AI will happily rely on existing tools it has been trained on, knowing exactly how to write their syntax. This eliminates the need to research documentation or figure out how to structure a codebase. Where this becomes a problem is when those tools are open source projects that rely on user traffic to their websites to advertise commercial offerings. One such example is Tailwind CSS. Despite being widely popular, it is now facing the possibility of becoming abandonware due to an inability to sustainably fund its maintenance team.\nIs this a precursor to what’s to come for Open Source Software (OSS)? Will some projects simply fade away after being abandoned by their communities?\nDisclaimer: OSS doesn’t always mean free — it means the source code is open. Anyone can inspect it, modify it, redistribute it, and use it. There may be limitations depending on the license associated with the project. However, the source code remains accessible, and in most cases, you are free to do what you want with it.\nThe OSS dilemma OSS collectively struggling with funding and contribution is nothing new. Some projects manage to buck this trend through specific revenue streams, but the majority rely on communities of developers coming together to make the software work. Most of this effort is time donated out of generosity, with contributors receiving very little in return. Rarely do developers make a sustainable living purely by contributing to OSS. Instead, most treat it as a hobby — something done in the evenings or on weekends, while juggling regular jobs and other commitments. While the cause is noble and honourable, it’s not going to pay the bills.\nFundamentally and in principle, OSS is great. It allows consumers to access software that would otherwise be unavailable to them. You need only look at your own device to see how many products you use every day that you didn’t directly pay for. For me, as a macOS user, this includes applications such as:\nHomebrew Docker/Colima Golang Hugo (website framework) Kind (Kubernetes sandpit) Terraform Pike (AWS IAM policy writer) Python Drawio Multipass (Local VM runner) Obsidian (Note taking app) Rectangle (Screen management) VS code From this list, some products have commercial offerings. Docker, for example, was originally completely open source but later began commercialising certain components, such as Docker Desktop. You can still run Docker “for free” so long as you use an alternative runtime like Colima.\nThis is where the OSS dilemma emerges. On one hand, we have access to an incredible suite of tools. On the other, those who maintain and contribute to these tools receive little in return beyond personal satisfaction and a sense of community. Participation alone, however meaningful, does not translate into long-term sustainability.\nTailwind CSS’ business model To work around the broader OSS funding dilemma, companies and maintainers are often forced to get creative. In the case of Tailwind CSS, revenue is primarily driven through:\nSponsorships A premium product (Tailwind Plus) Most OSS projects adopt a similar approach, offering paid options that unlock additional features or convenience. From the earlier list, Docker offers Docker Desktop, Terraform has been steadily moving toward more restrictive free-tier options following HashiCorp’s acquisition by IBM, and both Obsidian and Rectangle provide premium tiers that unlock extra functionality.\nDespite these offerings, returning to the earlier point about AI not needing—or caring about—these monetisation mechanisms, the companies behind such tools can still struggle. In December 2025, Tailwind CSS laid off 75% of its engineering team. That figure sounds dramatic, but it’s worth remembering that it represents a percentage rather than an absolute number. In practice, this reportedly amounted to three engineers losing their jobs.\nAt face value, this may not seem groundbreaking. However, consider how much reliance we place on these tools. Without OSS, we would be nowhere near as technologically advanced as we are today, and the digital age would look markedly different.\nAn AI tool is here to get a job done. Whatever prompt it is given, it will strive to achieve that goal using the information that it has available. If the tool that it relies on works, and the pattern is repeatable rather than esoteric, it becomes trivially easy for an AI model to generate the code you need—without ever engaging with the ecosystem that sustains it.\nWhen adoption doesn’t equal growth As AI becomes a prominent part of the software creation process, it bypasses the traditional systems and mechanisms that once drove revenue. Without those pathways, some projects may simply be unsustainable and be unable to continue. This creates a strange phenomenon: adoption without growth. In most other contexts, this would be unheard of. Consider a traditional business selling cosmetics. The products are a hit — widely popular among consumers — and the business expands accordingly. Revenue climbs, the company scales, and founders and executives are well compensated as the organisation thrives. This is what we typically expect when a product succeeds. Open source breaks this assumption. Consumers often expect that a product they obtained for free will simply continue to exist. They can continue to use it, without having to pay for it, and even profit from it. This is a deeply polarising aspect of OSS. Software that was freely obtained is used to generate revenue — sometimes significant revenue — while the project enabling that success quietly struggles to survive.\nIn what logical sense is that fair? Getting something for nothing, using it for personal or commercial gain, and continuing to expect it to persist indefinitely? It feels (and is) unreasonable when examined closely. Yet this is exactly the dynamic at play — and one that AI now accelerates rather than alleviates.\nNot an isolated issue Any OSS project that relies on human interaction with its resources to continue to operate may eventually face the same fate. Tailwind CSS is simply the catalyst. As AI continues to abstract users away from documentation, communities, and project ecosystems, more OSS projects may struggle to sustain themselves and ultimately fold.\nThis should be a consideration for how we use OSS in the future. Some projects may already be in trouble without any publicity. It could be something that is happening behind the scenes. What is the most concerning here is that OSS is used in many different capacities. It is used at an individual level, at a small business level, and extends all the way to the largest corporations — often using the exact same tools. Yet corporations, collectively, are unlikely to donate back to these projects. They have little incentive to. If the product is on offer for free, why would they? Imagine how that conversation would go with the financial team when it came time to raise a purchase order. It affects the bottom line, and if there is anything organisations always need more of, it\u0026rsquo;s money!\nTurning point and the future of OSS Tailwind CSS faces an uncertain future. Further reports are yet to emerge on how it is tracking financially or what its future holds. Sources suggest that some larger corporations and individuals who rely on Tailwind CSS have stepped forward to offer donations. However, there are concerns about how much is actually being raised and whether it is sufficient to pull Tailwind out of its current financial rut. Time will tell how it all plays out — and whether the internet will eventually need another CSS framework in the future when doing code refactors. Hopefully, one that can monetise itself effectively and remain operational.\nSo what does this mean for the future of OSS? It should serve as a warning. Not all projects are killing it, and some maintenance teams may have a heavier dependency on those projects than others. We all need to eat. Skilled software engineers are in demand — not those who simply “vibe code”, but the actual ones who nowadays leverage AI to make their code better and tackle their workload efficiently.\nOSS will continue to be as it was. In the event that a maintainer of a major project is unable to continue, the project either finds another one or it stops being supported. It\u0026rsquo;s happened before, and it will happen again. A recent example of this is the Ingress NGINX project. As the saying goes for the World of Warcraft geeks — No king rules forever.\nCover image by Artlist.io\n","date":"2026-02-02T10:05:32+10:00","image":"/p/tailwind-css-a-warning-for-open-source-projects-in-the-age-of-ai/cover.png","permalink":"/p/tailwind-css-a-warning-for-open-source-projects-in-the-age-of-ai/","title":"Tailwind CSS: A warning for open source projects in the age of AI"},{"content":"Introduction AWS networking is fairly straightforward in a small setup. As a singular environment, you will generally have a single VPC that consists of:\nSubnets Route tables Network ACLs (NACLs) Security groups An Internet Gateway (IGW) NAT Gateway(s) There are other components, and even some of the ones mentioned here may not be required. You might have a single subnet for all of your resources. Likewise, a NAT Gateway may not even be necessary (this is rare — and anyone exposing private workloads to the internet unnecessarily is not following best practice).\nBut what about in more sophisticated setups, where you have multiple isolated networks in the form of several VPCs across different AWS accounts? You may need to introduce a shared services layer — an isolated VPC (or multiple, depending on your use case) that houses applications used to support your environment. Rather than duplicating these components inside each VPC, we share them (hence the name). To share these components, the VPCs need to be interconnected. AWS provides a service called Transit Gateway (TGW) to achieve exactly that, at scale.\nBuilding on the shared services concept, NAT Gateways provide a solution that allows private subnets to connect to the internet for outbound (egress) traffic. They are deployed into public subnets (with a default route to an IGW) so that resources can route through the NAT Gateway to access the internet. NAT Gateways aren’t strictly bound to the VPC they are deployed in — you can use them across different VPCs if you choose, by leveraging a TGW.\nWith all of this context in mind, at what point does it make sense to use a NAT Gateway in each VPC (decentralised egress) versus using a TGW to route all internet traffic through a single VPC and set of NAT Gateways (centralised egress)? Which option is more cost-effective? Let’s find out.\nArchitecture: Centralised vs decentralised Let’s begin by outlining each of the solutions and what they look like architecturally.\nCentralised VPC count: 4 Workload VPCs: 3 Egress VPC: 1 Transit Gateway count: 1 Transit Gateway attachments: 4 Transit Gateway route tables: 1 NAT Gateway count: 2 (both deployed into the Egress VPC) Each VPC will be associated with a single Transit Gateway (TGW) route table and propagate its routes accordingly. A default route will be configured to point to the egress VPC, enabling a centralised egress pattern.\nEach VPC will be created in a separate AWS account, except for the egress VPC, which (along with the TGW) will reside in a dedicated networking account. The TGW will be shared via AWS Resource Access Manager (RAM) to each respective AWS account, allowing TGW attachments to be created for each VPC.\nThis is what the architecture looks like: Decentralised VPC count: 3 (Workload VPCs only) Transit Gateway count: 1 Transit Gateway attachments: 3 Transit Gateway route tables: 1 NAT Gateway count: 6 (2 in each workload VPC) Each VPC will be associated with a single TGW route table and will propagate its routes accordingly. This route table will not contain a default route, as traffic will exit locally via each VPCs NAT Gateways before reaching the TGW.\nEach VPC will be created in separate AWS accounts. The TGW will again be created in the networking account and RAM shared to each AWS account to allow for TGW attachments per VPC.\nThis is what the architecture looks like: Now that the architectures are defined, let’s dig deeper into the pricing of each component.\nPricing models NAT Gateways NAT Gateways have a simple pricing model. You pay for two main things:\nHourly cost per NAT Gateway Data processed (in GB) by the NAT Gateway The source or destination of the traffic is irrelevant — if the data passes through the NAT Gateway, you\u0026rsquo;re paying for it.\nIn addition to NAT Gateway costs, there are data transfer fees for services like EC2. This includes:\nTraffic going out to the internet Traffic sent to other AWS regions Traffic sent between Availability Zones (AZs) within the same region It’s also worth mentioning that VPCs themselves are free. While components inside a VPC (like NAT Gateways) may incur costs, the VPC construct has no cost on its own. So for this comparison, we’ll only factor in the pricing for NAT Gateways.\nTransit Gateways Transit Gateways follow a similar pricing model to NAT Gateways. You pay for:\nHourly cost per Transit Gateway attachment Data processed (in GB) by the Transit Gateway As with NAT Gateways, data transfer fees for services like EC2 still apply and should be considered — especially for inter-AZ or inter-region traffic.\nCost breakdown To calculate actual costs, we need to use the pricing from an AWS region. For this exercise, we’ll use the AWS Sydney region (ap-southeast-2). Based on this region, the pricing is as follows:\nNAT Gateway Hourly fee: $0.059 Per GB of data processed: $0.059 Transit Gateway: Hourly fee (per attachment): $0.07 Per GB of data processed: $0.02 For both the centralised and decentralised solutions already outlined, we’ll assume 100 GB of data processed in total. We’ll also assume a billing duration of 30 days (720 hours) for the hourly costs.\nAlthough both solutions will incur data transfer fees (e.g. to the internet, to other AWS regions, or between AZs within a region), these are identical in both cases. As such, we’ve excluded them from this comparison to focus only on the NAT and Transit Gateway pricing.\nCentralised cost breakdown Component Cost NAT Gateway 1 (hourly fee) $42.48 NAT Gateway 2 (hourly fee) $42.48 NAT Gateway (data processing) $5.90 Transit Gateway (Egress VPC attachment) $50.40 Transit Gateway (Workload A VPC attachment) $50.40 Transit Gateway (Workload B VPC attachment) $50.40 Transit Gateway (Workload C VPC attachment) $50.40 Transit Gateway (data processing) $2 Total $294.46 Decentralised cost breakdown Component Cost NAT Gateway 1 (hourly fee) $42.48 NAT Gateway 2 (hourly fee) $42.48 NAT Gateway 3 (hourly fee) $42.48 NAT Gateway 4 (hourly fee) $42.48 NAT Gateway 5 (hourly fee) $42.48 NAT Gateway 6 (hourly fee) $42.48 NAT Gateway (data processing) $5.90 Transit Gateway (Workload A VPC attachment) $50.40 Transit Gateway (Workload B VPC attachment) $50.40 Transit Gateway (Workload C VPC attachment) $50.40 Transit Gateway (data processing) $2 Total $413.98 Findings With the findings in tow, it\u0026rsquo;s clear which solution is more cost-effective based on the defined setup. The centralised egress solution comes out ahead in terms of cost when compared to the decentralised one — as long as the assumptions hold.\nHowever, there are edge cases worth exploring. Also, note that we used a \u0026ldquo;magic number\u0026rdquo; of 6 NAT Gateways in the decentralised setup — something we’ll expand on shortly.\nWhat if fewer VPCs needed egress? Suppose only one of the three workload VPCs in the decentralised solution actually required egress access (and therefore NAT Gateways). If you removed 4 of the 6 NAT Gateways, your total would drop to $244.06 — cheaper than the centralised solution.\nWhat If You Removed the Transit Gateway? Now imagine removing the Transit Gateway from the decentralised solution entirely, and each of the three VPCs had their own local NAT Gateways, but no interconnection. In that case, the total cost would come to $260.78 — not as low as the previous example, but still cheaper than the centralised solution (at $294.46).\nWhy We Used 3 VPCs as a Baseline The reason we compared three workload VPCs and 6 NAT Gateways is that this configuration represents a cost-efficiency threshold. Beyond this point, the decentralised approach starts to suffer from diminishing returns.\nWhile it may seem obvious to adopt a centralised solution, there\u0026rsquo;s a case to be made for starting with decentralised egress — especially in early-stage AWS environments that aren’t yet interconnected. If the VPCs remain isolated, decentralised egress might be more cost-effective. But as soon as inter-VPC connectivity is required, the decentralised model becomes significantly less economical.\nAdditional cost scenarios Here are some expanded scenarios to help illustrate the cost impact at scale:\nScenario Total Cost Decentralised: 4 VPCs, 8 NAT Gateways, 0 TGW $345.74 Centralised: 4 VPCs + 1 egress VPC, 2 NAT Gateways, 5 TGW attachments $344.86 Decentralised: 5 VPCs, 10 NAT Gateways, 0 TGW $430.70 Centralised: 5 VPCs + 1 egress VPC, 2 NAT Gateways, 6 TGW attachments $395.26 As shown above, once you pass three VPCs, the centralised model quickly becomes more attractive from a cost perspective.\nCentralised solution benefits While the primary focus of this blog is cost, not everything comes down to price. Ultimately, the real question is: what are you willing to pay for best practice and a secure, scalable setup?\nAWS provides several services that enhance security at the networking layer. Two key examples include:\nAWS Network Firewall – a fully managed stateful firewall service Gateway Load Balancer (GWLB) – supports third-party firewalls in a “bump-in-the-wire” architecture Both of these services are designed to operate in a centralised model, enabling consistent inspection of traffic — whether it\u0026rsquo;s north/south (in and out of the AWS network) or east/west (between VPCs/subnets). Any traffic that bypasses these centralised inspection points risks being unauthorised or even malicious. Without a centralised architecture, enforcing an inspection solution across multiple VPCs becomes near impossible.\nIf you needed any more reasons to lean toward a centralised model — regardless of cost — the ability to integrate services like AWS Network Firewall and Gateway Load Balancer should be near the top of the list.\nConclusion Egress traffic in AWS must traverse some kind of gateway to reach the internet. For private subnets, this means using a NAT Gateway. As we’ve explored, NAT Gateways can either be local to each VPC or located in a centralised egress VPC, connected via a Transit Gateway.\nCentralised solutions work best at scale and offer stronger alignment with AWS’s security services. Decentralised solutions, on the other hand, are often more suitable for smaller environments — either early in their cloud journey or intentionally minimal in scope.\nThe next time you\u0026rsquo;re designing how traffic should leave your AWS environment, consider the long-term direction, cost implications, security posture, and — most importantly — what the environment needs to function effectively.\nCover image generated by DeepAI\n","date":"2025-07-08T22:00:32+10:00","image":"/p/designing-egress-in-aws/cover.jpeg","permalink":"/p/designing-egress-in-aws/","title":"Designing Egress in AWS"},{"content":"Introduction Designing IT solutions is a challenging task. Not only do you need to take several factors into consideration, but you also need to understand the lifecycle of what you’re designing. Questions will inevitably pop into your head, like:\nHow long does this solution need to last? Is it going to be perpetual? How do we handle high availability and disaster recovery? When will I know it\u0026rsquo;s complete? That last point—when will I know it\u0026rsquo;s complete?—can be ambiguous. Does it mean the solution is fully fleshed out and working at the desired end state? Or does it mean it\u0026rsquo;s ready once a Minimum Viable Product (MVP) has been constructed?\nIf the solution is too simple, it may lack features or fail to scale. But if it’s too complex—over-engineered—it might introduce unexpected bugs and hurt not just our credibility, but also the brand we represent, especially in a corporate context. And beyond the bugs, what about the cost of building that solution? Our time? Money? Resources? All potentially wasted on something that doesn’t deliver a meaningful return on investment.\nIt’s a conundrum, no doubt. But there are frameworks and considerations that help you draw the line and confidently mark a solution as done based on criteria.\nUnderstanding a solution One of the first fundamental skills that you need to understand as a Consultant or anyone in a role that involves designing or architecting solutions—is the ability to understand the what and the why:\nWhat are we doing? What are we building? What is it? What will it be? Once we understand that what, we move on to the why:\nWhy are we doing this? Why are we building this? Why do we need to include x, y and z? Why is this or that a good idea? All of those what and why questions can be summarised in a single word: requirements. Requirements are the driving force behind the rationale. They help shape what the solution will look like and give context to the decisions made along the way. They set the direction—establishing goals, coordinating efforts across teams, and managing expectations for stakeholders. From a commercial perspective, requirements are assessed and translated into elements such as definition of done and/or acceptance criteria. These elements are then used in commercial agreements like a Statement of Work (SoW) to provide clear and concise deliverables for all parties involved.\nWhen we understand the requirements of what we’re setting out to do, we have structure. That structure becomes the foundation for a plan—where ideas take shape and strategy begins to form. As you become more proficient in your craft—whether it’s software, infrastructure, or something else—you’ll begin to recognise patterns. You’ll recall what worked well and what didn’t in previous experiences. Armed with that insight, you’ll feel more confident when similar situations arise, and better equipped to act early—before issues have a chance to fester.\nThe definition of complete Now comes the tricky part of this process: What defines a solution as complete?\nIs it something that we can measure? Does it relate to its run cost? Can it handle the load we need it to? Will it accumulate technical debt? We do have frameworks and tools to help answer these questions. In the previous section on understanding a solution, we mentioned the definition of done and acceptance criteria. Depending on how mature the environment and organisation are, these could be loosely defined or meticulously detailed. By using these elements, we can clearly describe what a “complete” solution looks like—aligned with what we’re trying to achieve now.\nEven though we use the term “complete”, it doesn’t always mean the end state. More often, it marks a meaningful checkpoint. The future state will continue to evolve—just as the broader IT landscape constantly shifts.\nA definition of done may reflect a simple, even traditional, solution. Solutions don’t need to be extravagant if they fulfill the basic requirements. There’s nothing wrong with doing things the old-fashioned way. Take mainframes, for example. You might think of them as legacy tech from the 1960s—but would it surprise you to know that many major corporations across sectors like finance, government, and healthcare still rely on mainframes today to process transactions? Mainframes continue to hold relevance because of their unique capabilities and the critical roles they play in these industries. Could they be replaced by modern stacks like serverless architectures? Probably. Is it a simple task? Absolutely not.\nThe mainframe example highlights two key principles:\nIf it isn’t broken, don’t fix it — or in other words, if it works, don’t change it. Old technology doesn’t automatically mean obsolete — in fact, as older generations retire, we might even see a kind of renaissance for legacy technologies like mainframes. At one point in history, mainframes were considered the north star of computing. Fast forward to today, and our modern-day north stars might be serverless platforms that run only your code—or perhaps AI-powered observability systems that proactively detect anomalies.\nTechnology doesn’t stand still. It evolves rapidly. Today, we’re seeing explosive growth in AI, particularly with agentic models that leverage Model Context Protocol (MCP) servers.\nSo when it comes to defining what makes a solution \u0026ldquo;complete,\u0026rdquo; remember these quick tidbits:\nIt’s fit for purpose. It doesn’t need to last forever—just for today, tomorrow, and the near future. Solutions will always evolve. It will scale within reason. If you’re expecting millions of requests per second, architect accordingly. But if you\u0026rsquo;re only handling a few per second, you don’t need a massive data platform with all the bells and whistles that might cost thousands per month to run. It will work. This is the core of it all. “Working” might mean different things to different people, but ultimately, the solution behaves as intended and delivers on its purpose. Over-engineering and its ramifications So what happens when we go to the other extreme? Rather than stopping at our definition of done, we push way past it and engineer the hell out of a solution. It’s amazing! It’s something to behold!\n…Or is it?\nHow far down the rabbit hole did you have to go to get there? And ultimately—what was the cost?\nOver-engineering is contextual. When it does happen, it can manifest in different forms depending on the solution. One of the worst forms is trying to account for every possible future use case—a “just in case” mindset. Building a solution to handle everything that might happen in the future can stifle progress. It can halt or delay the project until there is mutual resolution amongst stakeholders, all while delaying progress to the end goal. In situations like this, I often use the saying: “the juice isn’t worth the squeeze.” In other words, to achieve X, we need to do Y—and Y just isn’t worth the investment of our time, resources, or mental energy.\nAnother form of over-engineering is adding unnecessary complexity. The maturity of a solution must be factored in when architecting. Introducing advanced complexity to a system that’s not ready for it can be wasteful, or even damaging. Take a startup, for example. Let’s say their core revenue-driving product is a piece of software. Right now, that software runs on a single PC under someone’s desk. Is it elegant? No. Does it work for now? Actually, yes—in a very non-ideal way.\nIf that same startup tries to launch into a cutting-edge, serverless cloud architecture with auto-scaling, CI/CD pipelines, and bleeding-edge automation—but has no current need for it—that effort would be a catastrophe. The resources required to build and support such a setup would likely never see a return, especially if the startup is still finding product-market fit or struggling with cash flow. In fact, they might run out of steam entirely and fold before realising their goals.\nA solution shouldn’t be too top-heavy at the start. By that, I mean: don’t add features or components that don’t bring actual value in the near term. Later—once things stabilize and there’s a clear path forward—it might make sense to revisit and layer in those “nice to have” features. But early on, they often just weigh you down.\nReturn on Investment (ROI) and building momentum After we get through the rigorous process of vetting the solution using the mechanisms already mentioned, we need to shift into a mindset that focuses on vision—the long-term outlook for the solution. As part of this vision, we must consider how we extract value over time and achieve a solid return on investment (ROI). But ROI isn’t always about money. There are other forms of currency we “spend” to make a solution functional and successful. These include:\nTime Effort On-going support Business integration Let’s start with time. It’s one of our most precious resources—yet most of us (myself included) never seem to have enough of it. In the digital age, it’s easy to jump from watching Judge Judy on YouTube to checking sports results, reading the latest tariff news, playing a video game, or falling into a social media rabbit hole. There’s always something waiting to eat into our time.\nSo, when we invest time into a solution, it should give us time back. The return should come in the form of efficiency or automation. This is especially true when adopting DevOps methodologies, where the whole premise is to automate as much of your role as possible—freeing you up for higher-value tasks.\nAnother often-overlooked factor is the longevity of the solution and what ongoing support and maintenance will look like. Simple systems might be seen as ancient by today’s standards—but they can also be the easiest to maintain, debug, and enhance. A “good enough” solution today often beats a “perfect” one six months from now.\nA solution that is well-vetted, tested, iterated upon, intuitive, and clearly defined delivers strong ROI—not just for development teams, but also for project managers, business stakeholders, and end users. When more people begin interacting with the solution, momentum builds. Feedback loops form, giving valuable insight into what to improve or prioritise next.\nThis is how momentum turns into progress. This is how “good enough” evolves into great.\nConclusion The next time you\u0026rsquo;re designing a solution—or playing a role in shaping one—consider what’s practical to get it working now, versus what’s “perfect” but loaded with nice-to-have features that may not add much value.\nDon’t get stuck in the weeds and chase those rabbits too deep into those holes! While it can be fun to experiment and theory-craft, a good engineer or architect knows when they\u0026rsquo;ve gone deep enough to achieve a solid result for a reasonable amount of effort. In the end, completing a solution up until a point isn’t settling. It\u0026rsquo;s being strategic.\nCover image by dix sept on Unsplash\n","date":"2025-06-12T22:20:32+10:00","image":"/photo-1624961151169-b3df5c0f06ab_8133815761830699768.jpg","permalink":"/p/perfect-vs-practical-how-to-know-when-an-it-solution-is-ready/","title":"Perfect vs Practical: How to know when an IT Solution is ready"}]